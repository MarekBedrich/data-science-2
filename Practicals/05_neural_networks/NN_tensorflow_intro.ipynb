{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow intro\n",
    "\n",
    "This tutorial shows the basic usage of tensorflow to train neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Read data and convert them to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=20000, n_features=8, n_informative=5, \n",
    "                           n_redundant=0, n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (12800, 8), target_ratio: 0.502\n",
      "test size: (4000, 8), target_ratio: 0.501\n",
      "dev size: (3200, 8), target_ratio: 0.502\n"
     ]
    }
   ],
   "source": [
    "print('train size: {}, target_ratio: {:.3f}'.format(X_train.shape, np.mean(y_train)))\n",
    "print('test size: {}, target_ratio: {:.3f}'.format(X_test.shape, np.mean(y_test)))\n",
    "print('dev size: {}, target_ratio: {:.3f}'.format(X_dev.shape, np.mean(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple model with tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very useful documentations with many examples and detailed explanation of everything you might need:\n",
    " - https://www.tensorflow.org/api_docs/python/tf/keras/\n",
    " - https://keras.io/api/\n",
    "\n",
    "Contain everything about:\n",
    "  - Model building: Activations, Losses, Optimizers, Regularization\n",
    "  - Data processing\n",
    "  - Pretrained models and datasets\n",
    "  - Automatic differentiation\n",
    "  - ...\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model speficication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three APIs for building the model\n",
    "   - sequential - easy to code, but less flexible - we will use it sometimes\n",
    "   - functional - flexible and still easy to code - we will use it the most\n",
    "   - model subclassing - rather complicated and not very much used - we will skip it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "\n",
    "Easy to code but <span style=\"color:red\"> NOT </span> appropriate when:\n",
    "\n",
    "- Your model has multiple inputs or multiple outputs\n",
    "- Any of your layers has multiple inputs or multiple outputs\n",
    "- You need to do layer sharing\n",
    "- You want non-linear topology (e.g. a residual connection, a multi-branch model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                90        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specification A)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer([X_train.shape[1],]), # Create input layer with 'input data' neurons\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), # Create hidden layer with 10 neurons and ReLU activation (RELU JE X NA R+, 0 NA R-)\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"), # Create output layer with one neuron and sigmoid activation\n",
    "]) # TEDY JEN INPUT X (BERE SE JAKO JEDEN INPUT), JEDEN LAYER S 10 NEURONAMA A OUTPUT CISLO\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# MYSLIM ZE 90 PARAMETRU V HIDDEN PROTOZE 10*8 (10 NEURONU A X MA 8 SLOUPCU) + 10 BIAS PARAMETRU. U OUTPUT PAK 10 Z NEURONU A 1 BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specification B) - TOTO NEDELAL, ALTERNATIVNI ZPUSOB JAK TO UDELAT\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://www.tensorflow.org/guide/keras/functional)\n",
    "\n",
    "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(X_train.shape[1],)) # V SEKVENCNIM NAPSAL TY LAYERS JAK JSOU ZA SEBOU, TADY ZADA CO TAM MA BYT A TEN MODEL SE NAVRHNE SAM\n",
    "\n",
    "hidden = tf.keras.layers.Dense(10)(inputs)\n",
    "hidden = tf.keras.activations.relu(hidden)\n",
    "hidden = tf.keras.layers.Dense(1)(hidden)\n",
    "outputs = tf.keras.activations.sigmoid(hidden)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " tf.nn.relu (TFOpLambda)     (None, 10)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      " tf.math.sigmoid (TFOpLambda  (None, 1)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# TODO: To make the following line work you need to install graphviz (if you have not done so in one of the previous classes)\n",
    "# 1) follow the instructions https://graphviz.gitlab.io/download/?fbclid=IwAR1V-lrRhho5rSfBVYXYISsighqRwOCOgMHLmL_DclkQrPtMXQaKj3mFcqs\n",
    "# 2) this notebook has been tested with version 8.0.3\n",
    "# 3) make sure you add it to the PATH variable (you are specifically asked during the installation) at least for local user\n",
    "\n",
    "tf.keras.utils.plot_model(model) # ASI PRCAM, neni to nejakej zazrak grafickej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZATIM Jsem zkonstruoval tu sit jakoby, ale porad mi chybi prpojeni s outputy, potrebuju LOSS funkci abych vyhodnotil jak daleko je nas output \n",
    "# od realneho. Pouzijeme binary cross entropy, binary referencuje typ outputu, cross entropy se pouziva kdyz je output pravdepodobnostni rozdeleni \n",
    "# a chceme merit jako daleko je prave Y od predikovaneho. C-E se pocita jako -E_{Y_true}(log(Y_pred)). Y_true je brano jako nahodne.\n",
    "# Realne teda pocitame -sum_{k=1}^N y_true^(k) * log(y_pred^(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with selected optimizer, loss and metrics\n",
    "model.compile(\n",
    "        optimizer=tf.optimizers.Adam(), # Several other possibilities for optimizers \n",
    "        loss=tf.losses.BinaryCrossentropy(), # Select the proper loss for the task\n",
    "        metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.BinaryAccuracy()], # Select the proper metrics for the task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Bias of the last layers:\n",
      "[0.]\n",
      "\n",
      ">>> Kernel of the last layers:\n",
      "[[-0.04433858]\n",
      " [-0.48884207]\n",
      " [-0.14795089]\n",
      " [ 0.49902743]\n",
      " [-0.43155372]\n",
      " [-0.34787712]\n",
      " [-0.45331258]\n",
      " [ 0.21712661]\n",
      " [-0.5472337 ]\n",
      " [ 0.41418248]]\n",
      "\n",
      ">>> Bias of the first layers:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      ">>> Kernel of the first layers:\n",
      "[[-0.13606709  0.28868985  0.20628262 -0.38618106  0.3443514   0.4406668\n",
      "  -0.4625658   0.3101338   0.02230495 -0.34151125]\n",
      " [ 0.34663117  0.47178292 -0.5677739   0.15289944 -0.17409372 -0.5652492\n",
      "   0.10102141 -0.33703676  0.31804562 -0.38921916]\n",
      " [-0.10551062 -0.14723098 -0.4651388   0.46210492 -0.44629174 -0.24044785\n",
      "   0.09708405  0.4910314   0.5610268  -0.08296752]\n",
      " [ 0.00611526 -0.22575524 -0.17058706 -0.31471995 -0.551751   -0.41084895\n",
      "   0.01788127  0.23742157  0.42155093  0.13149142]\n",
      " [ 0.09500605 -0.4300148  -0.45128986 -0.5492053  -0.3490768   0.57227325\n",
      "   0.20611864 -0.4884621  -0.4377318  -0.06032985]\n",
      " [ 0.38897717 -0.5671009  -0.07463712 -0.20924884 -0.31475216 -0.57716334\n",
      "  -0.5147007  -0.43470195 -0.26023036 -0.25736502]\n",
      " [-0.22249031 -0.47490114 -0.486879    0.25312936  0.36313593  0.39370686\n",
      "  -0.3944656  -0.52504    -0.5274464   0.3231011 ]\n",
      " [ 0.10193557  0.42825234  0.49372137 -0.0339092  -0.18049559 -0.5240991\n",
      "   0.04551542 -0.16596887  0.2032404   0.4247048 ]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n>>> Bias of the last layers:')\n",
    "print(model.layers[3].weights[1].numpy())\n",
    "\n",
    "print('\\n>>> Kernel of the last layers:')\n",
    "print(model.layers[3].weights[0].numpy())\n",
    "\n",
    "print('\\n>>> Bias of the first layers:')\n",
    "print(model.layers[1].weights[1].numpy())\n",
    "\n",
    "print('\\n>>> Kernel of the first layers:')\n",
    "print(model.layers[1].weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 1s 2ms/step - loss: 0.7509 - auc: 0.5350 - binary_accuracy: 0.5270\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5519 - auc: 0.8141 - binary_accuracy: 0.7460\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4330 - auc: 0.9034 - binary_accuracy: 0.8234\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 997us/step - loss: 0.3682 - auc: 0.9267 - binary_accuracy: 0.8537\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 985us/step - loss: 0.3311 - auc: 0.9388 - binary_accuracy: 0.8711\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 963us/step - loss: 0.3076 - auc: 0.9459 - binary_accuracy: 0.8817\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 964us/step - loss: 0.2926 - auc: 0.9502 - binary_accuracy: 0.8880\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 947us/step - loss: 0.2830 - auc: 0.9530 - binary_accuracy: 0.8909\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 941us/step - loss: 0.2756 - auc: 0.9552 - binary_accuracy: 0.8939\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 976us/step - loss: 0.2699 - auc: 0.9568 - binary_accuracy: 0.8958\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 928us/step - loss: 0.2650 - auc: 0.9583 - binary_accuracy: 0.8978\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 961us/step - loss: 0.2614 - auc: 0.9593 - binary_accuracy: 0.8991\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 891us/step - loss: 0.2583 - auc: 0.9602 - binary_accuracy: 0.9006\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 980us/step - loss: 0.2559 - auc: 0.9608 - binary_accuracy: 0.9002\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 947us/step - loss: 0.2538 - auc: 0.9614 - binary_accuracy: 0.9003\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 941us/step - loss: 0.2522 - auc: 0.9618 - binary_accuracy: 0.9014\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 923us/step - loss: 0.2508 - auc: 0.9622 - binary_accuracy: 0.9018\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 943us/step - loss: 0.2495 - auc: 0.9626 - binary_accuracy: 0.9016\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 974us/step - loss: 0.2484 - auc: 0.9629 - binary_accuracy: 0.9013\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 926us/step - loss: 0.2475 - auc: 0.9632 - binary_accuracy: 0.9023\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 892us/step - loss: 0.2467 - auc: 0.9633 - binary_accuracy: 0.9028\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 941us/step - loss: 0.2456 - auc: 0.9636 - binary_accuracy: 0.9027\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 973us/step - loss: 0.2451 - auc: 0.9638 - binary_accuracy: 0.9032\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 934us/step - loss: 0.2439 - auc: 0.9641 - binary_accuracy: 0.9039\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2431 - auc: 0.9644 - binary_accuracy: 0.9047\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 938us/step - loss: 0.2423 - auc: 0.9646 - binary_accuracy: 0.9045\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 915us/step - loss: 0.2415 - auc: 0.9648 - binary_accuracy: 0.9048\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 933us/step - loss: 0.2406 - auc: 0.9651 - binary_accuracy: 0.9053\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 933us/step - loss: 0.2397 - auc: 0.9654 - binary_accuracy: 0.9055\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 883us/step - loss: 0.2388 - auc: 0.9657 - binary_accuracy: 0.9060\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 913us/step - loss: 0.2379 - auc: 0.9660 - binary_accuracy: 0.9069\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 961us/step - loss: 0.2369 - auc: 0.9663 - binary_accuracy: 0.9079\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 932us/step - loss: 0.2361 - auc: 0.9666 - binary_accuracy: 0.9058\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 955us/step - loss: 0.2352 - auc: 0.9669 - binary_accuracy: 0.9077\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 957us/step - loss: 0.2341 - auc: 0.9672 - binary_accuracy: 0.9083\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 962us/step - loss: 0.2330 - auc: 0.9676 - binary_accuracy: 0.9088\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2317 - auc: 0.9679 - binary_accuracy: 0.9092\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 928us/step - loss: 0.2308 - auc: 0.9682 - binary_accuracy: 0.9096\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 894us/step - loss: 0.2299 - auc: 0.9685 - binary_accuracy: 0.9106\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 928us/step - loss: 0.2287 - auc: 0.9688 - binary_accuracy: 0.9101\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2277 - auc: 0.9692 - binary_accuracy: 0.9107\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 906us/step - loss: 0.2270 - auc: 0.9694 - binary_accuracy: 0.9112\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 868us/step - loss: 0.2258 - auc: 0.9698 - binary_accuracy: 0.9116\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 929us/step - loss: 0.2249 - auc: 0.9700 - binary_accuracy: 0.9119\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 962us/step - loss: 0.2240 - auc: 0.9703 - binary_accuracy: 0.9124\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 887us/step - loss: 0.2234 - auc: 0.9704 - binary_accuracy: 0.9123\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 910us/step - loss: 0.2222 - auc: 0.9709 - binary_accuracy: 0.9142\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 963us/step - loss: 0.2215 - auc: 0.9711 - binary_accuracy: 0.9138\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 941us/step - loss: 0.2204 - auc: 0.9715 - binary_accuracy: 0.9150\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 963us/step - loss: 0.2197 - auc: 0.9716 - binary_accuracy: 0.9152\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 934us/step - loss: 0.2187 - auc: 0.9719 - binary_accuracy: 0.9158\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 907us/step - loss: 0.2181 - auc: 0.9722 - binary_accuracy: 0.9162\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 915us/step - loss: 0.2171 - auc: 0.9725 - binary_accuracy: 0.9165\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 967us/step - loss: 0.2162 - auc: 0.9728 - binary_accuracy: 0.9170\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 882us/step - loss: 0.2153 - auc: 0.9730 - binary_accuracy: 0.9167\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 930us/step - loss: 0.2147 - auc: 0.9731 - binary_accuracy: 0.9183\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 988us/step - loss: 0.2137 - auc: 0.9734 - binary_accuracy: 0.9192\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 912us/step - loss: 0.2129 - auc: 0.9736 - binary_accuracy: 0.9199\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 951us/step - loss: 0.2120 - auc: 0.9739 - binary_accuracy: 0.9204\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2117 - auc: 0.9740 - binary_accuracy: 0.9202\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 935us/step - loss: 0.2106 - auc: 0.9743 - binary_accuracy: 0.9212\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 928us/step - loss: 0.2097 - auc: 0.9746 - binary_accuracy: 0.9224\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 885us/step - loss: 0.2088 - auc: 0.9748 - binary_accuracy: 0.9224\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 912us/step - loss: 0.2079 - auc: 0.9751 - binary_accuracy: 0.9232\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 937us/step - loss: 0.2070 - auc: 0.9754 - binary_accuracy: 0.9238\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 969us/step - loss: 0.2058 - auc: 0.9758 - binary_accuracy: 0.9253\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 962us/step - loss: 0.2047 - auc: 0.9760 - binary_accuracy: 0.9265\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 970us/step - loss: 0.2031 - auc: 0.9766 - binary_accuracy: 0.9266\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 907us/step - loss: 0.2015 - auc: 0.9770 - binary_accuracy: 0.9294\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 998us/step - loss: 0.2002 - auc: 0.9775 - binary_accuracy: 0.9285\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1983 - auc: 0.9780 - binary_accuracy: 0.9315\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 958us/step - loss: 0.1965 - auc: 0.9785 - binary_accuracy: 0.9320\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 937us/step - loss: 0.1956 - auc: 0.9786 - binary_accuracy: 0.9330\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 929us/step - loss: 0.1939 - auc: 0.9792 - binary_accuracy: 0.9331\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 960us/step - loss: 0.1927 - auc: 0.9794 - binary_accuracy: 0.9329\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 991us/step - loss: 0.1916 - auc: 0.9798 - binary_accuracy: 0.9358\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 921us/step - loss: 0.1898 - auc: 0.9802 - binary_accuracy: 0.9348\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 963us/step - loss: 0.1891 - auc: 0.9803 - binary_accuracy: 0.9365\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 951us/step - loss: 0.1880 - auc: 0.9804 - binary_accuracy: 0.9356\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 966us/step - loss: 0.1869 - auc: 0.9808 - binary_accuracy: 0.9360\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 882us/step - loss: 0.1860 - auc: 0.9808 - binary_accuracy: 0.9365\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 951us/step - loss: 0.1853 - auc: 0.9810 - binary_accuracy: 0.9379\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 942us/step - loss: 0.1842 - auc: 0.9811 - binary_accuracy: 0.9384\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 951us/step - loss: 0.1830 - auc: 0.9814 - binary_accuracy: 0.9394\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 980us/step - loss: 0.1823 - auc: 0.9816 - binary_accuracy: 0.9395\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 967us/step - loss: 0.1817 - auc: 0.9817 - binary_accuracy: 0.9392\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 993us/step - loss: 0.1809 - auc: 0.9818 - binary_accuracy: 0.9404\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 936us/step - loss: 0.1802 - auc: 0.9820 - binary_accuracy: 0.9411\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 972us/step - loss: 0.1793 - auc: 0.9822 - binary_accuracy: 0.9420\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1788 - auc: 0.9823 - binary_accuracy: 0.9412\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1777 - auc: 0.9825 - binary_accuracy: 0.9419\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1772 - auc: 0.9825 - binary_accuracy: 0.9414\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1764 - auc: 0.9826 - binary_accuracy: 0.9429\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1760 - auc: 0.9827 - binary_accuracy: 0.9422\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 999us/step - loss: 0.1756 - auc: 0.9827 - binary_accuracy: 0.9436\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1750 - auc: 0.9828 - binary_accuracy: 0.9438\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 994us/step - loss: 0.1740 - auc: 0.9830 - binary_accuracy: 0.9437\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1735 - auc: 0.9831 - binary_accuracy: 0.9441\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1730 - auc: 0.9832 - binary_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1723 - auc: 0.9832 - binary_accuracy: 0.9436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf981cfc10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with default setting\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=100) # BATCH SIZE JE KOLIK INPUTU POUZIJU NA VYPOCET LOSS, TYPICKY HODNOTA 64, MENE NE, MAX TREBA 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 2ms/step - loss: 0.1907 - auc: 0.9795 - binary_accuracy: 0.9423\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and predict for the test data\n",
    "model.evaluate(X_test, y_test)\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 0.36\n",
      "0 - 0.33\n",
      "0 - 0.34\n",
      "1 - 1.00\n",
      "0 - 0.00\n",
      "0 - 0.04\n",
      "1 - 0.97\n",
      "0 - 0.01\n",
      "1 - 0.97\n",
      "1 - 1.00\n"
     ]
    }
   ],
   "source": [
    "for pred, true in zip(test_pred, y_test[0:10]):\n",
    "    print('{} - {:.2f}'.format(true, pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add early stopping and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RegularizedModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " tf.nn.relu_3 (TFOpLambda)   (None, 10)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      " tf.math.sigmoid_3 (TFOpLamb  (None, 1)                0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[1]))\n",
    "\n",
    "# Hidden layer with regularization and ReLU\n",
    "hidden = tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs) # l2 REGULARIZACE\n",
    "hidden = tf.keras.activations.relu(hidden)\n",
    "\n",
    "# Output layer with regularization and sigmoid\n",
    "outputs = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.001))(hidden)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='RegularizedModel')\n",
    "\n",
    "model.compile(\n",
    "        optimizer=tf.optimizers.Adam(),\n",
    "        loss=tf.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "200/200 [==============================] - 1s 2ms/step - loss: 0.6542 - auc_3: 0.7018 - binary_accuracy: 0.6420 - val_loss: 0.5757 - val_auc_3: 0.8194 - val_binary_accuracy: 0.7344\n",
      "Epoch 2/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5190 - auc_3: 0.8551 - binary_accuracy: 0.7692 - val_loss: 0.4559 - val_auc_3: 0.8934 - val_binary_accuracy: 0.8050\n",
      "Epoch 3/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4324 - auc_3: 0.9005 - binary_accuracy: 0.8163 - val_loss: 0.3917 - val_auc_3: 0.9197 - val_binary_accuracy: 0.8425\n",
      "Epoch 4/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3841 - auc_3: 0.9215 - binary_accuracy: 0.8429 - val_loss: 0.3570 - val_auc_3: 0.9332 - val_binary_accuracy: 0.8584\n",
      "Epoch 5/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3568 - auc_3: 0.9322 - binary_accuracy: 0.8577 - val_loss: 0.3397 - val_auc_3: 0.9391 - val_binary_accuracy: 0.8681\n",
      "Epoch 6/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3416 - auc_3: 0.9379 - binary_accuracy: 0.8673 - val_loss: 0.3289 - val_auc_3: 0.9427 - val_binary_accuracy: 0.8737\n",
      "Epoch 7/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3319 - auc_3: 0.9414 - binary_accuracy: 0.8738 - val_loss: 0.3218 - val_auc_3: 0.9454 - val_binary_accuracy: 0.8766\n",
      "Epoch 8/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3247 - auc_3: 0.9441 - binary_accuracy: 0.8784 - val_loss: 0.3163 - val_auc_3: 0.9475 - val_binary_accuracy: 0.8816\n",
      "Epoch 9/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3190 - auc_3: 0.9463 - binary_accuracy: 0.8833 - val_loss: 0.3116 - val_auc_3: 0.9493 - val_binary_accuracy: 0.8853\n",
      "Epoch 10/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3142 - auc_3: 0.9482 - binary_accuracy: 0.8857 - val_loss: 0.3081 - val_auc_3: 0.9507 - val_binary_accuracy: 0.8872\n",
      "Epoch 11/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3099 - auc_3: 0.9499 - binary_accuracy: 0.8911 - val_loss: 0.3046 - val_auc_3: 0.9523 - val_binary_accuracy: 0.8925\n",
      "Epoch 12/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3059 - auc_3: 0.9516 - binary_accuracy: 0.8929 - val_loss: 0.3015 - val_auc_3: 0.9535 - val_binary_accuracy: 0.8922\n",
      "Epoch 13/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3024 - auc_3: 0.9529 - binary_accuracy: 0.8969 - val_loss: 0.2987 - val_auc_3: 0.9546 - val_binary_accuracy: 0.8966\n",
      "Epoch 14/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2988 - auc_3: 0.9544 - binary_accuracy: 0.8992 - val_loss: 0.2954 - val_auc_3: 0.9559 - val_binary_accuracy: 0.8991\n",
      "Epoch 15/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2953 - auc_3: 0.9560 - binary_accuracy: 0.9012 - val_loss: 0.2929 - val_auc_3: 0.9565 - val_binary_accuracy: 0.9013\n",
      "Epoch 16/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2923 - auc_3: 0.9572 - binary_accuracy: 0.9020 - val_loss: 0.2897 - val_auc_3: 0.9577 - val_binary_accuracy: 0.9031\n",
      "Epoch 17/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2893 - auc_3: 0.9584 - binary_accuracy: 0.9039 - val_loss: 0.2871 - val_auc_3: 0.9588 - val_binary_accuracy: 0.9053\n",
      "Epoch 18/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2864 - auc_3: 0.9595 - binary_accuracy: 0.9053 - val_loss: 0.2850 - val_auc_3: 0.9599 - val_binary_accuracy: 0.9056\n",
      "Epoch 19/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2839 - auc_3: 0.9606 - binary_accuracy: 0.9076 - val_loss: 0.2834 - val_auc_3: 0.9610 - val_binary_accuracy: 0.9072\n",
      "Epoch 20/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2814 - auc_3: 0.9617 - binary_accuracy: 0.9084 - val_loss: 0.2806 - val_auc_3: 0.9615 - val_binary_accuracy: 0.9094\n",
      "Epoch 21/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2799 - auc_3: 0.9621 - binary_accuracy: 0.9087 - val_loss: 0.2788 - val_auc_3: 0.9625 - val_binary_accuracy: 0.9084\n",
      "Epoch 22/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2777 - auc_3: 0.9630 - binary_accuracy: 0.9093 - val_loss: 0.2777 - val_auc_3: 0.9630 - val_binary_accuracy: 0.9084\n",
      "Epoch 23/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2761 - auc_3: 0.9636 - binary_accuracy: 0.9102 - val_loss: 0.2754 - val_auc_3: 0.9640 - val_binary_accuracy: 0.9094\n",
      "Epoch 24/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2743 - auc_3: 0.9646 - binary_accuracy: 0.9116 - val_loss: 0.2735 - val_auc_3: 0.9643 - val_binary_accuracy: 0.9103\n",
      "Epoch 25/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2727 - auc_3: 0.9651 - binary_accuracy: 0.9118 - val_loss: 0.2715 - val_auc_3: 0.9650 - val_binary_accuracy: 0.9128\n",
      "Epoch 26/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2714 - auc_3: 0.9654 - binary_accuracy: 0.9109 - val_loss: 0.2713 - val_auc_3: 0.9660 - val_binary_accuracy: 0.9109\n",
      "Epoch 27/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2701 - auc_3: 0.9662 - binary_accuracy: 0.9122 - val_loss: 0.2703 - val_auc_3: 0.9661 - val_binary_accuracy: 0.9119\n",
      "Epoch 28/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2688 - auc_3: 0.9666 - binary_accuracy: 0.9128 - val_loss: 0.2680 - val_auc_3: 0.9664 - val_binary_accuracy: 0.9147\n",
      "Epoch 29/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2677 - auc_3: 0.9670 - binary_accuracy: 0.9137 - val_loss: 0.2672 - val_auc_3: 0.9668 - val_binary_accuracy: 0.9131\n",
      "Epoch 30/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2666 - auc_3: 0.9673 - binary_accuracy: 0.9130 - val_loss: 0.2666 - val_auc_3: 0.9675 - val_binary_accuracy: 0.9122\n",
      "Epoch 31/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2654 - auc_3: 0.9680 - binary_accuracy: 0.9130 - val_loss: 0.2665 - val_auc_3: 0.9669 - val_binary_accuracy: 0.9153\n",
      "Epoch 32/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2648 - auc_3: 0.9680 - binary_accuracy: 0.9140 - val_loss: 0.2645 - val_auc_3: 0.9681 - val_binary_accuracy: 0.9128\n",
      "Epoch 33/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2638 - auc_3: 0.9684 - binary_accuracy: 0.9145 - val_loss: 0.2642 - val_auc_3: 0.9684 - val_binary_accuracy: 0.9128\n",
      "Epoch 34/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2632 - auc_3: 0.9688 - binary_accuracy: 0.9153 - val_loss: 0.2638 - val_auc_3: 0.9679 - val_binary_accuracy: 0.9147\n",
      "Epoch 35/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2629 - auc_3: 0.9686 - binary_accuracy: 0.9160 - val_loss: 0.2627 - val_auc_3: 0.9688 - val_binary_accuracy: 0.9119\n",
      "Epoch 36/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2619 - auc_3: 0.9692 - binary_accuracy: 0.9153 - val_loss: 0.2624 - val_auc_3: 0.9687 - val_binary_accuracy: 0.9159\n",
      "Epoch 37/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2614 - auc_3: 0.9693 - binary_accuracy: 0.9148 - val_loss: 0.2612 - val_auc_3: 0.9690 - val_binary_accuracy: 0.9166\n",
      "Epoch 38/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2607 - auc_3: 0.9696 - binary_accuracy: 0.9166 - val_loss: 0.2611 - val_auc_3: 0.9695 - val_binary_accuracy: 0.9153\n",
      "Epoch 39/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2601 - auc_3: 0.9700 - binary_accuracy: 0.9172 - val_loss: 0.2599 - val_auc_3: 0.9693 - val_binary_accuracy: 0.9166\n",
      "Epoch 40/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2594 - auc_3: 0.9699 - binary_accuracy: 0.9167 - val_loss: 0.2608 - val_auc_3: 0.9699 - val_binary_accuracy: 0.9125\n",
      "Epoch 41/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2589 - auc_3: 0.9704 - binary_accuracy: 0.9166 - val_loss: 0.2594 - val_auc_3: 0.9695 - val_binary_accuracy: 0.9172\n",
      "Epoch 42/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2588 - auc_3: 0.9704 - binary_accuracy: 0.9166 - val_loss: 0.2586 - val_auc_3: 0.9700 - val_binary_accuracy: 0.9178\n",
      "Epoch 43/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2581 - auc_3: 0.9705 - binary_accuracy: 0.9193 - val_loss: 0.2591 - val_auc_3: 0.9704 - val_binary_accuracy: 0.9147\n",
      "Epoch 44/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2575 - auc_3: 0.9708 - binary_accuracy: 0.9180 - val_loss: 0.2598 - val_auc_3: 0.9704 - val_binary_accuracy: 0.9147\n",
      "Epoch 45/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2574 - auc_3: 0.9708 - binary_accuracy: 0.9188 - val_loss: 0.2590 - val_auc_3: 0.9703 - val_binary_accuracy: 0.9147\n",
      "Epoch 46/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2567 - auc_3: 0.9709 - binary_accuracy: 0.9190 - val_loss: 0.2574 - val_auc_3: 0.9707 - val_binary_accuracy: 0.9194\n",
      "Epoch 47/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2566 - auc_3: 0.9711 - binary_accuracy: 0.9195 - val_loss: 0.2568 - val_auc_3: 0.9710 - val_binary_accuracy: 0.9197\n",
      "Epoch 48/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2560 - auc_3: 0.9711 - binary_accuracy: 0.9178 - val_loss: 0.2580 - val_auc_3: 0.9712 - val_binary_accuracy: 0.9156\n",
      "Epoch 49/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2560 - auc_3: 0.9713 - binary_accuracy: 0.9195 - val_loss: 0.2581 - val_auc_3: 0.9710 - val_binary_accuracy: 0.9125\n",
      "Epoch 50/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2555 - auc_3: 0.9716 - binary_accuracy: 0.9190 - val_loss: 0.2586 - val_auc_3: 0.9711 - val_binary_accuracy: 0.9166\n",
      "Epoch 51/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2550 - auc_3: 0.9717 - binary_accuracy: 0.9200 - val_loss: 0.2569 - val_auc_3: 0.9708 - val_binary_accuracy: 0.9187\n",
      "Epoch 52/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2546 - auc_3: 0.9716 - binary_accuracy: 0.9194 - val_loss: 0.2555 - val_auc_3: 0.9713 - val_binary_accuracy: 0.9209\n",
      "Epoch 53/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2543 - auc_3: 0.9719 - binary_accuracy: 0.9196 - val_loss: 0.2558 - val_auc_3: 0.9708 - val_binary_accuracy: 0.9203\n",
      "Epoch 54/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2542 - auc_3: 0.9719 - binary_accuracy: 0.9202 - val_loss: 0.2552 - val_auc_3: 0.9712 - val_binary_accuracy: 0.9209\n",
      "Epoch 55/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2539 - auc_3: 0.9721 - binary_accuracy: 0.9201 - val_loss: 0.2552 - val_auc_3: 0.9713 - val_binary_accuracy: 0.9203\n",
      "Epoch 56/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2537 - auc_3: 0.9721 - binary_accuracy: 0.9212 - val_loss: 0.2556 - val_auc_3: 0.9717 - val_binary_accuracy: 0.9178\n",
      "Epoch 57/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2532 - auc_3: 0.9723 - binary_accuracy: 0.9212 - val_loss: 0.2551 - val_auc_3: 0.9717 - val_binary_accuracy: 0.9191\n",
      "Epoch 58/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2529 - auc_3: 0.9724 - binary_accuracy: 0.9207 - val_loss: 0.2560 - val_auc_3: 0.9720 - val_binary_accuracy: 0.9175\n",
      "Epoch 59/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2524 - auc_3: 0.9727 - binary_accuracy: 0.9212 - val_loss: 0.2544 - val_auc_3: 0.9717 - val_binary_accuracy: 0.9212\n",
      "Epoch 60/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2524 - auc_3: 0.9725 - binary_accuracy: 0.9218 - val_loss: 0.2544 - val_auc_3: 0.9718 - val_binary_accuracy: 0.9200\n",
      "Epoch 61/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2521 - auc_3: 0.9726 - binary_accuracy: 0.9218 - val_loss: 0.2535 - val_auc_3: 0.9720 - val_binary_accuracy: 0.9219\n",
      "Epoch 62/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2517 - auc_3: 0.9727 - binary_accuracy: 0.9205 - val_loss: 0.2536 - val_auc_3: 0.9726 - val_binary_accuracy: 0.9212\n",
      "Epoch 63/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2515 - auc_3: 0.9729 - binary_accuracy: 0.9215 - val_loss: 0.2538 - val_auc_3: 0.9724 - val_binary_accuracy: 0.9194\n",
      "Epoch 64/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2512 - auc_3: 0.9730 - binary_accuracy: 0.9220 - val_loss: 0.2544 - val_auc_3: 0.9723 - val_binary_accuracy: 0.9197\n",
      "Epoch 65/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2512 - auc_3: 0.9728 - binary_accuracy: 0.9235 - val_loss: 0.2552 - val_auc_3: 0.9727 - val_binary_accuracy: 0.9197\n",
      "Epoch 66/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2510 - auc_3: 0.9730 - binary_accuracy: 0.9226 - val_loss: 0.2530 - val_auc_3: 0.9727 - val_binary_accuracy: 0.9216\n",
      "Epoch 67/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2502 - auc_3: 0.9732 - binary_accuracy: 0.9225 - val_loss: 0.2565 - val_auc_3: 0.9727 - val_binary_accuracy: 0.9162\n",
      "Epoch 68/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2503 - auc_3: 0.9734 - binary_accuracy: 0.9236 - val_loss: 0.2536 - val_auc_3: 0.9728 - val_binary_accuracy: 0.9194\n",
      "Epoch 69/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2505 - auc_3: 0.9733 - binary_accuracy: 0.9234 - val_loss: 0.2546 - val_auc_3: 0.9725 - val_binary_accuracy: 0.9175\n",
      "Epoch 70/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2497 - auc_3: 0.9737 - binary_accuracy: 0.9238 - val_loss: 0.2538 - val_auc_3: 0.9726 - val_binary_accuracy: 0.9216\n",
      "Epoch 71/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2494 - auc_3: 0.9734 - binary_accuracy: 0.9232 - val_loss: 0.2530 - val_auc_3: 0.9729 - val_binary_accuracy: 0.9231\n",
      "Epoch 72/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2495 - auc_3: 0.9737 - binary_accuracy: 0.9247 - val_loss: 0.2524 - val_auc_3: 0.9728 - val_binary_accuracy: 0.9209\n",
      "Epoch 73/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2493 - auc_3: 0.9736 - binary_accuracy: 0.9250 - val_loss: 0.2520 - val_auc_3: 0.9732 - val_binary_accuracy: 0.9216\n",
      "Epoch 74/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2490 - auc_3: 0.9738 - binary_accuracy: 0.9238 - val_loss: 0.2513 - val_auc_3: 0.9734 - val_binary_accuracy: 0.9237\n",
      "Epoch 75/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2489 - auc_3: 0.9738 - binary_accuracy: 0.9252 - val_loss: 0.2514 - val_auc_3: 0.9732 - val_binary_accuracy: 0.9228\n",
      "Epoch 76/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2484 - auc_3: 0.9739 - binary_accuracy: 0.9251 - val_loss: 0.2517 - val_auc_3: 0.9733 - val_binary_accuracy: 0.9216\n",
      "Epoch 77/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2481 - auc_3: 0.9741 - binary_accuracy: 0.9250 - val_loss: 0.2517 - val_auc_3: 0.9733 - val_binary_accuracy: 0.9222\n",
      "Epoch 78/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2481 - auc_3: 0.9741 - binary_accuracy: 0.9254 - val_loss: 0.2512 - val_auc_3: 0.9731 - val_binary_accuracy: 0.9222\n",
      "Epoch 79/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2480 - auc_3: 0.9741 - binary_accuracy: 0.9255 - val_loss: 0.2504 - val_auc_3: 0.9735 - val_binary_accuracy: 0.9241\n",
      "Epoch 80/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2474 - auc_3: 0.9743 - binary_accuracy: 0.9254 - val_loss: 0.2510 - val_auc_3: 0.9731 - val_binary_accuracy: 0.9231\n",
      "Epoch 81/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2475 - auc_3: 0.9742 - binary_accuracy: 0.9257 - val_loss: 0.2516 - val_auc_3: 0.9738 - val_binary_accuracy: 0.9200\n",
      "Epoch 82/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2474 - auc_3: 0.9744 - binary_accuracy: 0.9253 - val_loss: 0.2497 - val_auc_3: 0.9739 - val_binary_accuracy: 0.9225\n",
      "Epoch 83/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2471 - auc_3: 0.9743 - binary_accuracy: 0.9260 - val_loss: 0.2503 - val_auc_3: 0.9739 - val_binary_accuracy: 0.9225\n",
      "Epoch 84/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2467 - auc_3: 0.9745 - binary_accuracy: 0.9249 - val_loss: 0.2496 - val_auc_3: 0.9742 - val_binary_accuracy: 0.9212\n",
      "Epoch 85/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2466 - auc_3: 0.9746 - binary_accuracy: 0.9260 - val_loss: 0.2504 - val_auc_3: 0.9739 - val_binary_accuracy: 0.9203\n",
      "Epoch 86/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2465 - auc_3: 0.9746 - binary_accuracy: 0.9259 - val_loss: 0.2491 - val_auc_3: 0.9740 - val_binary_accuracy: 0.9234\n",
      "Epoch 87/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2463 - auc_3: 0.9746 - binary_accuracy: 0.9259 - val_loss: 0.2506 - val_auc_3: 0.9741 - val_binary_accuracy: 0.9187\n",
      "Epoch 88/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2462 - auc_3: 0.9747 - binary_accuracy: 0.9266 - val_loss: 0.2498 - val_auc_3: 0.9739 - val_binary_accuracy: 0.9200\n",
      "Epoch 89/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2459 - auc_3: 0.9749 - binary_accuracy: 0.9272 - val_loss: 0.2487 - val_auc_3: 0.9742 - val_binary_accuracy: 0.9228\n",
      "Epoch 90/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2457 - auc_3: 0.9748 - binary_accuracy: 0.9262 - val_loss: 0.2483 - val_auc_3: 0.9744 - val_binary_accuracy: 0.9197\n",
      "Epoch 91/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2456 - auc_3: 0.9749 - binary_accuracy: 0.9263 - val_loss: 0.2485 - val_auc_3: 0.9746 - val_binary_accuracy: 0.9203\n",
      "Epoch 92/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2450 - auc_3: 0.9752 - binary_accuracy: 0.9277 - val_loss: 0.2484 - val_auc_3: 0.9742 - val_binary_accuracy: 0.9219\n",
      "Epoch 93/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2452 - auc_3: 0.9750 - binary_accuracy: 0.9269 - val_loss: 0.2480 - val_auc_3: 0.9748 - val_binary_accuracy: 0.9203\n",
      "Epoch 94/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2449 - auc_3: 0.9751 - binary_accuracy: 0.9278 - val_loss: 0.2482 - val_auc_3: 0.9748 - val_binary_accuracy: 0.9228\n",
      "Epoch 95/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2451 - auc_3: 0.9752 - binary_accuracy: 0.9268 - val_loss: 0.2486 - val_auc_3: 0.9750 - val_binary_accuracy: 0.9194\n",
      "Epoch 96/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2447 - auc_3: 0.9751 - binary_accuracy: 0.9279 - val_loss: 0.2482 - val_auc_3: 0.9750 - val_binary_accuracy: 0.9209\n",
      "Epoch 97/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2449 - auc_3: 0.9753 - binary_accuracy: 0.9263 - val_loss: 0.2477 - val_auc_3: 0.9747 - val_binary_accuracy: 0.9219\n",
      "Epoch 98/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2446 - auc_3: 0.9753 - binary_accuracy: 0.9273 - val_loss: 0.2478 - val_auc_3: 0.9746 - val_binary_accuracy: 0.9225\n",
      "Epoch 99/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2444 - auc_3: 0.9754 - binary_accuracy: 0.9275 - val_loss: 0.2472 - val_auc_3: 0.9749 - val_binary_accuracy: 0.9247\n",
      "Epoch 100/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2444 - auc_3: 0.9752 - binary_accuracy: 0.9269 - val_loss: 0.2476 - val_auc_3: 0.9751 - val_binary_accuracy: 0.9191\n",
      "Epoch 101/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2445 - auc_3: 0.9753 - binary_accuracy: 0.9278 - val_loss: 0.2471 - val_auc_3: 0.9750 - val_binary_accuracy: 0.9225\n",
      "Epoch 102/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2440 - auc_3: 0.9754 - binary_accuracy: 0.9277 - val_loss: 0.2468 - val_auc_3: 0.9748 - val_binary_accuracy: 0.9228\n",
      "Epoch 103/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2440 - auc_3: 0.9755 - binary_accuracy: 0.9277 - val_loss: 0.2463 - val_auc_3: 0.9751 - val_binary_accuracy: 0.9250\n",
      "Epoch 104/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2440 - auc_3: 0.9754 - binary_accuracy: 0.9276 - val_loss: 0.2465 - val_auc_3: 0.9754 - val_binary_accuracy: 0.9244\n",
      "Epoch 105/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2437 - auc_3: 0.9756 - binary_accuracy: 0.9285 - val_loss: 0.2472 - val_auc_3: 0.9753 - val_binary_accuracy: 0.9231\n",
      "Epoch 106/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2435 - auc_3: 0.9756 - binary_accuracy: 0.9281 - val_loss: 0.2468 - val_auc_3: 0.9755 - val_binary_accuracy: 0.9231\n",
      "Epoch 107/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2435 - auc_3: 0.9756 - binary_accuracy: 0.9287 - val_loss: 0.2488 - val_auc_3: 0.9752 - val_binary_accuracy: 0.9219\n",
      "Epoch 108/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2432 - auc_3: 0.9756 - binary_accuracy: 0.9290 - val_loss: 0.2469 - val_auc_3: 0.9754 - val_binary_accuracy: 0.9219\n",
      "Epoch 109/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2430 - auc_3: 0.9758 - binary_accuracy: 0.9279 - val_loss: 0.2458 - val_auc_3: 0.9753 - val_binary_accuracy: 0.9234\n",
      "Epoch 110/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2429 - auc_3: 0.9758 - binary_accuracy: 0.9282 - val_loss: 0.2466 - val_auc_3: 0.9754 - val_binary_accuracy: 0.9244\n",
      "Epoch 111/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2430 - auc_3: 0.9757 - binary_accuracy: 0.9287 - val_loss: 0.2462 - val_auc_3: 0.9755 - val_binary_accuracy: 0.9234\n",
      "Epoch 112/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2427 - auc_3: 0.9757 - binary_accuracy: 0.9279 - val_loss: 0.2469 - val_auc_3: 0.9756 - val_binary_accuracy: 0.9197\n",
      "Epoch 113/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2428 - auc_3: 0.9759 - binary_accuracy: 0.9277 - val_loss: 0.2460 - val_auc_3: 0.9754 - val_binary_accuracy: 0.9247\n",
      "Epoch 114/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2428 - auc_3: 0.9757 - binary_accuracy: 0.9277 - val_loss: 0.2457 - val_auc_3: 0.9757 - val_binary_accuracy: 0.9203\n",
      "Epoch 115/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2425 - auc_3: 0.9759 - binary_accuracy: 0.9290 - val_loss: 0.2460 - val_auc_3: 0.9756 - val_binary_accuracy: 0.9222\n",
      "Epoch 116/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2427 - auc_3: 0.9758 - binary_accuracy: 0.9282 - val_loss: 0.2484 - val_auc_3: 0.9757 - val_binary_accuracy: 0.9184\n",
      "Epoch 117/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2421 - auc_3: 0.9760 - binary_accuracy: 0.9280 - val_loss: 0.2451 - val_auc_3: 0.9760 - val_binary_accuracy: 0.9228\n",
      "Epoch 118/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2421 - auc_3: 0.9760 - binary_accuracy: 0.9295 - val_loss: 0.2480 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9203\n",
      "Epoch 119/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2424 - auc_3: 0.9760 - binary_accuracy: 0.9293 - val_loss: 0.2451 - val_auc_3: 0.9755 - val_binary_accuracy: 0.9250\n",
      "Epoch 120/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2425 - auc_3: 0.9758 - binary_accuracy: 0.9278 - val_loss: 0.2451 - val_auc_3: 0.9757 - val_binary_accuracy: 0.9247\n",
      "Epoch 121/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2418 - auc_3: 0.9761 - binary_accuracy: 0.9281 - val_loss: 0.2460 - val_auc_3: 0.9756 - val_binary_accuracy: 0.9203\n",
      "Epoch 122/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2418 - auc_3: 0.9761 - binary_accuracy: 0.9280 - val_loss: 0.2445 - val_auc_3: 0.9756 - val_binary_accuracy: 0.9266\n",
      "Epoch 123/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2418 - auc_3: 0.9759 - binary_accuracy: 0.9294 - val_loss: 0.2451 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9250\n",
      "Epoch 124/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2419 - auc_3: 0.9761 - binary_accuracy: 0.9288 - val_loss: 0.2459 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9219\n",
      "Epoch 125/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2416 - auc_3: 0.9761 - binary_accuracy: 0.9282 - val_loss: 0.2454 - val_auc_3: 0.9757 - val_binary_accuracy: 0.9234\n",
      "Epoch 126/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2415 - auc_3: 0.9761 - binary_accuracy: 0.9291 - val_loss: 0.2476 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9191\n",
      "Epoch 127/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2415 - auc_3: 0.9762 - binary_accuracy: 0.9283 - val_loss: 0.2473 - val_auc_3: 0.9755 - val_binary_accuracy: 0.9203\n",
      "Epoch 128/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2412 - auc_3: 0.9761 - binary_accuracy: 0.9281 - val_loss: 0.2471 - val_auc_3: 0.9761 - val_binary_accuracy: 0.9191\n",
      "Epoch 129/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2414 - auc_3: 0.9761 - binary_accuracy: 0.9287 - val_loss: 0.2455 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9200\n",
      "Epoch 130/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2410 - auc_3: 0.9764 - binary_accuracy: 0.9287 - val_loss: 0.2451 - val_auc_3: 0.9756 - val_binary_accuracy: 0.9212\n",
      "Epoch 131/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2412 - auc_3: 0.9762 - binary_accuracy: 0.9277 - val_loss: 0.2440 - val_auc_3: 0.9760 - val_binary_accuracy: 0.9237\n",
      "Epoch 132/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2408 - auc_3: 0.9763 - binary_accuracy: 0.9284 - val_loss: 0.2445 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9228\n",
      "Epoch 133/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2411 - auc_3: 0.9762 - binary_accuracy: 0.9282 - val_loss: 0.2439 - val_auc_3: 0.9759 - val_binary_accuracy: 0.9237\n",
      "Epoch 134/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2408 - auc_3: 0.9763 - binary_accuracy: 0.9277 - val_loss: 0.2438 - val_auc_3: 0.9760 - val_binary_accuracy: 0.9253\n",
      "Epoch 135/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2411 - auc_3: 0.9763 - binary_accuracy: 0.9288 - val_loss: 0.2442 - val_auc_3: 0.9762 - val_binary_accuracy: 0.9225\n",
      "Epoch 136/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2406 - auc_3: 0.9763 - binary_accuracy: 0.9280 - val_loss: 0.2463 - val_auc_3: 0.9764 - val_binary_accuracy: 0.9225\n",
      "Epoch 137/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2407 - auc_3: 0.9765 - binary_accuracy: 0.9289 - val_loss: 0.2442 - val_auc_3: 0.9758 - val_binary_accuracy: 0.9234\n",
      "Epoch 138/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2408 - auc_3: 0.9763 - binary_accuracy: 0.9275 - val_loss: 0.2439 - val_auc_3: 0.9762 - val_binary_accuracy: 0.9225\n",
      "Epoch 139/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2406 - auc_3: 0.9764 - binary_accuracy: 0.9287 - val_loss: 0.2441 - val_auc_3: 0.9762 - val_binary_accuracy: 0.9209\n",
      "Epoch 140/500\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2407 - auc_3: 0.9765 - binary_accuracy: 0.9281 - val_loss: 0.2440 - val_auc_3: 0.9764 - val_binary_accuracy: 0.9234\n",
      "Epoch 141/500\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2404 - auc_3: 0.9765 - binary_accuracy: 0.9281 - val_loss: 0.2457 - val_auc_3: 0.9764 - val_binary_accuracy: 0.9216\n",
      "Epoch 142/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2405 - auc_3: 0.9765 - binary_accuracy: 0.9281 - val_loss: 0.2444 - val_auc_3: 0.9764 - val_binary_accuracy: 0.9228\n",
      "Epoch 143/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2401 - auc_3: 0.9765 - binary_accuracy: 0.9282 - val_loss: 0.2441 - val_auc_3: 0.9762 - val_binary_accuracy: 0.9253\n",
      "Epoch 144/500\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2403 - auc_3: 0.9765 - binary_accuracy: 0.9280 - val_loss: 0.2453 - val_auc_3: 0.9766 - val_binary_accuracy: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cfa53454e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "early_call = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_call])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch and Tensorboard - JAK TEDA NAJIT SPRAVNE HYPERMARAMETRY?\n",
    "Run gridsearch over hidden layer size, L2 regularization, activation, check the outputs in Tensorboard\n",
    "\n",
    "I recommend not to run Tensorboard from Jupyter notebook but from terminal directly\n",
    "\n",
    "use \"tensorboard --logdir logs\" in command line\n",
    "\n",
    "PRO RUZNE KOMBINACE NATRENUJE MODELY S NIZSIM POCTEM EPOCH A PAK SROVNA VYKONNOST POMOCI TENSORFLOW UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7762 - AUC: 0.6801 - binary_accuracy: 0.6295 - val_loss: 0.6788 - val_AUC: 0.7462 - val_binary_accuracy: 0.6591\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6243 - AUC: 0.8022 - binary_accuracy: 0.6895 - val_loss: 0.5844 - val_AUC: 0.8442 - val_binary_accuracy: 0.7225\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5624 - AUC: 0.8562 - binary_accuracy: 0.7573 - val_loss: 0.5412 - val_AUC: 0.8754 - val_binary_accuracy: 0.7897\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5332 - AUC: 0.8738 - binary_accuracy: 0.7911 - val_loss: 0.5190 - val_AUC: 0.8867 - val_binary_accuracy: 0.8081\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5172 - AUC: 0.8791 - binary_accuracy: 0.7945 - val_loss: 0.5057 - val_AUC: 0.8906 - val_binary_accuracy: 0.8128\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5067 - AUC: 0.8822 - binary_accuracy: 0.7969 - val_loss: 0.4960 - val_AUC: 0.8934 - val_binary_accuracy: 0.8144\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4988 - AUC: 0.8834 - binary_accuracy: 0.7987 - val_loss: 0.4886 - val_AUC: 0.8942 - val_binary_accuracy: 0.8166\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4924 - AUC: 0.8855 - binary_accuracy: 0.8057 - val_loss: 0.4825 - val_AUC: 0.8955 - val_binary_accuracy: 0.8231\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4870 - AUC: 0.8872 - binary_accuracy: 0.8098 - val_loss: 0.4771 - val_AUC: 0.8962 - val_binary_accuracy: 0.8309\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4824 - AUC: 0.8884 - binary_accuracy: 0.8151 - val_loss: 0.4727 - val_AUC: 0.8972 - val_binary_accuracy: 0.8338\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4784 - AUC: 0.8895 - binary_accuracy: 0.8199 - val_loss: 0.4685 - val_AUC: 0.8980 - val_binary_accuracy: 0.8400\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4750 - AUC: 0.8902 - binary_accuracy: 0.8227 - val_loss: 0.4652 - val_AUC: 0.8990 - val_binary_accuracy: 0.8422\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4721 - AUC: 0.8916 - binary_accuracy: 0.8250 - val_loss: 0.4628 - val_AUC: 0.8989 - val_binary_accuracy: 0.8419\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4697 - AUC: 0.8916 - binary_accuracy: 0.8263 - val_loss: 0.4601 - val_AUC: 0.9007 - val_binary_accuracy: 0.8453\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4679 - AUC: 0.8926 - binary_accuracy: 0.8300 - val_loss: 0.4583 - val_AUC: 0.9007 - val_binary_accuracy: 0.8444\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4662 - AUC: 0.8929 - binary_accuracy: 0.8316 - val_loss: 0.4568 - val_AUC: 0.9009 - val_binary_accuracy: 0.8441\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4650 - AUC: 0.8933 - binary_accuracy: 0.8310 - val_loss: 0.4552 - val_AUC: 0.9019 - val_binary_accuracy: 0.8478\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4639 - AUC: 0.8939 - binary_accuracy: 0.8328 - val_loss: 0.4539 - val_AUC: 0.9027 - val_binary_accuracy: 0.8475\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4631 - AUC: 0.8946 - binary_accuracy: 0.8330 - val_loss: 0.4529 - val_AUC: 0.9032 - val_binary_accuracy: 0.8472\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4622 - AUC: 0.8957 - binary_accuracy: 0.8328 - val_loss: 0.4521 - val_AUC: 0.9038 - val_binary_accuracy: 0.8478\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4615 - AUC: 0.8956 - binary_accuracy: 0.8345 - val_loss: 0.4511 - val_AUC: 0.9042 - val_binary_accuracy: 0.8494\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4608 - AUC: 0.8967 - binary_accuracy: 0.8347 - val_loss: 0.4503 - val_AUC: 0.9049 - val_binary_accuracy: 0.8487\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4604 - AUC: 0.8973 - binary_accuracy: 0.8333 - val_loss: 0.4497 - val_AUC: 0.9054 - val_binary_accuracy: 0.8497\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4600 - AUC: 0.8977 - binary_accuracy: 0.8357 - val_loss: 0.4490 - val_AUC: 0.9057 - val_binary_accuracy: 0.8503\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4597 - AUC: 0.8977 - binary_accuracy: 0.8333 - val_loss: 0.4488 - val_AUC: 0.9061 - val_binary_accuracy: 0.8512\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4593 - AUC: 0.8984 - binary_accuracy: 0.8357 - val_loss: 0.4482 - val_AUC: 0.9065 - val_binary_accuracy: 0.8512\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4591 - AUC: 0.8986 - binary_accuracy: 0.8354 - val_loss: 0.4479 - val_AUC: 0.9070 - val_binary_accuracy: 0.8506\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4588 - AUC: 0.8987 - binary_accuracy: 0.8355 - val_loss: 0.4475 - val_AUC: 0.9071 - val_binary_accuracy: 0.8506\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4586 - AUC: 0.8990 - binary_accuracy: 0.8352 - val_loss: 0.4472 - val_AUC: 0.9071 - val_binary_accuracy: 0.8509\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4585 - AUC: 0.8991 - binary_accuracy: 0.8353 - val_loss: 0.4470 - val_AUC: 0.9073 - val_binary_accuracy: 0.8500\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7298 - AUC: 0.6591 - binary_accuracy: 0.5796 - val_loss: 0.6442 - val_AUC: 0.7943 - val_binary_accuracy: 0.7125\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5889 - AUC: 0.8471 - binary_accuracy: 0.7603 - val_loss: 0.5307 - val_AUC: 0.8963 - val_binary_accuracy: 0.7956\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5030 - AUC: 0.8950 - binary_accuracy: 0.7968 - val_loss: 0.4718 - val_AUC: 0.9145 - val_binary_accuracy: 0.8281\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4659 - AUC: 0.9115 - binary_accuracy: 0.8239 - val_loss: 0.4472 - val_AUC: 0.9222 - val_binary_accuracy: 0.8444\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4473 - AUC: 0.9204 - binary_accuracy: 0.8438 - val_loss: 0.4326 - val_AUC: 0.9295 - val_binary_accuracy: 0.8584\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4359 - AUC: 0.9268 - binary_accuracy: 0.8561 - val_loss: 0.4249 - val_AUC: 0.9336 - val_binary_accuracy: 0.8672\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4292 - AUC: 0.9304 - binary_accuracy: 0.8627 - val_loss: 0.4200 - val_AUC: 0.9360 - val_binary_accuracy: 0.8675\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4248 - AUC: 0.9327 - binary_accuracy: 0.8648 - val_loss: 0.4174 - val_AUC: 0.9376 - val_binary_accuracy: 0.8703\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4215 - AUC: 0.9347 - binary_accuracy: 0.8662 - val_loss: 0.4149 - val_AUC: 0.9392 - val_binary_accuracy: 0.8712\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4191 - AUC: 0.9365 - binary_accuracy: 0.8684 - val_loss: 0.4129 - val_AUC: 0.9406 - val_binary_accuracy: 0.8731\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4171 - AUC: 0.9378 - binary_accuracy: 0.8705 - val_loss: 0.4115 - val_AUC: 0.9411 - val_binary_accuracy: 0.8759\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4156 - AUC: 0.9388 - binary_accuracy: 0.8716 - val_loss: 0.4098 - val_AUC: 0.9420 - val_binary_accuracy: 0.8766\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4140 - AUC: 0.9399 - binary_accuracy: 0.8726 - val_loss: 0.4089 - val_AUC: 0.9427 - val_binary_accuracy: 0.8794\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4130 - AUC: 0.9407 - binary_accuracy: 0.8737 - val_loss: 0.4083 - val_AUC: 0.9431 - val_binary_accuracy: 0.8794\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4120 - AUC: 0.9416 - binary_accuracy: 0.8734 - val_loss: 0.4069 - val_AUC: 0.9440 - val_binary_accuracy: 0.8806\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4113 - AUC: 0.9417 - binary_accuracy: 0.8752 - val_loss: 0.4065 - val_AUC: 0.9446 - val_binary_accuracy: 0.8822\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4106 - AUC: 0.9423 - binary_accuracy: 0.8745 - val_loss: 0.4061 - val_AUC: 0.9449 - val_binary_accuracy: 0.8822\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4100 - AUC: 0.9428 - binary_accuracy: 0.8770 - val_loss: 0.4067 - val_AUC: 0.9451 - val_binary_accuracy: 0.8784\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4097 - AUC: 0.9432 - binary_accuracy: 0.8768 - val_loss: 0.4059 - val_AUC: 0.9449 - val_binary_accuracy: 0.8822\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4093 - AUC: 0.9434 - binary_accuracy: 0.8772 - val_loss: 0.4057 - val_AUC: 0.9455 - val_binary_accuracy: 0.8834\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4094 - AUC: 0.9436 - binary_accuracy: 0.8788 - val_loss: 0.4055 - val_AUC: 0.9456 - val_binary_accuracy: 0.8841\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4090 - AUC: 0.9438 - binary_accuracy: 0.8782 - val_loss: 0.4057 - val_AUC: 0.9454 - val_binary_accuracy: 0.8825\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4090 - AUC: 0.9439 - binary_accuracy: 0.8793 - val_loss: 0.4055 - val_AUC: 0.9456 - val_binary_accuracy: 0.8828\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4089 - AUC: 0.9439 - binary_accuracy: 0.8787 - val_loss: 0.4057 - val_AUC: 0.9457 - val_binary_accuracy: 0.8834\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4089 - AUC: 0.9440 - binary_accuracy: 0.8796 - val_loss: 0.4053 - val_AUC: 0.9457 - val_binary_accuracy: 0.8841\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4089 - AUC: 0.9441 - binary_accuracy: 0.8796 - val_loss: 0.4051 - val_AUC: 0.9458 - val_binary_accuracy: 0.8831\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4087 - AUC: 0.9440 - binary_accuracy: 0.8794 - val_loss: 0.4056 - val_AUC: 0.9461 - val_binary_accuracy: 0.8850\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4086 - AUC: 0.9444 - binary_accuracy: 0.8797 - val_loss: 0.4055 - val_AUC: 0.9456 - val_binary_accuracy: 0.8822\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4087 - AUC: 0.9442 - binary_accuracy: 0.8802 - val_loss: 0.4052 - val_AUC: 0.9460 - val_binary_accuracy: 0.8831\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4086 - AUC: 0.9441 - binary_accuracy: 0.8790 - val_loss: 0.4056 - val_AUC: 0.9462 - val_binary_accuracy: 0.8863\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.8266 - AUC: 0.3657 - binary_accuracy: 0.3649 - val_loss: 0.7887 - val_AUC: 0.3768 - val_binary_accuracy: 0.3781\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7580 - AUC: 0.4243 - binary_accuracy: 0.4377 - val_loss: 0.7349 - val_AUC: 0.4684 - val_binary_accuracy: 0.5128\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7132 - AUC: 0.5324 - binary_accuracy: 0.5690 - val_loss: 0.6949 - val_AUC: 0.5901 - val_binary_accuracy: 0.5950\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6700 - AUC: 0.6865 - binary_accuracy: 0.6451 - val_loss: 0.6404 - val_AUC: 0.7720 - val_binary_accuracy: 0.7031\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6109 - AUC: 0.8097 - binary_accuracy: 0.7445 - val_loss: 0.5816 - val_AUC: 0.8387 - val_binary_accuracy: 0.7778\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5704 - AUC: 0.8335 - binary_accuracy: 0.7728 - val_loss: 0.5499 - val_AUC: 0.8446 - val_binary_accuracy: 0.7916\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5488 - AUC: 0.8369 - binary_accuracy: 0.7822 - val_loss: 0.5315 - val_AUC: 0.8479 - val_binary_accuracy: 0.7984\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5362 - AUC: 0.8398 - binary_accuracy: 0.7844 - val_loss: 0.5209 - val_AUC: 0.8488 - val_binary_accuracy: 0.8019\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5288 - AUC: 0.8402 - binary_accuracy: 0.7859 - val_loss: 0.5143 - val_AUC: 0.8491 - val_binary_accuracy: 0.8047\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5241 - AUC: 0.8420 - binary_accuracy: 0.7872 - val_loss: 0.5102 - val_AUC: 0.8498 - val_binary_accuracy: 0.8087\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5211 - AUC: 0.8427 - binary_accuracy: 0.7884 - val_loss: 0.5069 - val_AUC: 0.8529 - val_binary_accuracy: 0.8100\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5182 - AUC: 0.8468 - binary_accuracy: 0.7879 - val_loss: 0.5036 - val_AUC: 0.8594 - val_binary_accuracy: 0.8103\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5145 - AUC: 0.8533 - binary_accuracy: 0.7891 - val_loss: 0.4994 - val_AUC: 0.8645 - val_binary_accuracy: 0.8109\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5100 - AUC: 0.8582 - binary_accuracy: 0.7895 - val_loss: 0.4951 - val_AUC: 0.8697 - val_binary_accuracy: 0.8112\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5054 - AUC: 0.8630 - binary_accuracy: 0.7904 - val_loss: 0.4912 - val_AUC: 0.8746 - val_binary_accuracy: 0.8094\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5013 - AUC: 0.8668 - binary_accuracy: 0.7911 - val_loss: 0.4880 - val_AUC: 0.8780 - val_binary_accuracy: 0.8100\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4979 - AUC: 0.8697 - binary_accuracy: 0.7916 - val_loss: 0.4854 - val_AUC: 0.8811 - val_binary_accuracy: 0.8103\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4952 - AUC: 0.8725 - binary_accuracy: 0.7934 - val_loss: 0.4833 - val_AUC: 0.8830 - val_binary_accuracy: 0.8087\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4932 - AUC: 0.8743 - binary_accuracy: 0.7925 - val_loss: 0.4821 - val_AUC: 0.8845 - val_binary_accuracy: 0.8081\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4918 - AUC: 0.8758 - binary_accuracy: 0.7936 - val_loss: 0.4811 - val_AUC: 0.8863 - val_binary_accuracy: 0.8078\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4908 - AUC: 0.8769 - binary_accuracy: 0.7955 - val_loss: 0.4805 - val_AUC: 0.8871 - val_binary_accuracy: 0.8100\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4904 - AUC: 0.8777 - binary_accuracy: 0.7932 - val_loss: 0.4803 - val_AUC: 0.8879 - val_binary_accuracy: 0.8081\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4899 - AUC: 0.8784 - binary_accuracy: 0.7944 - val_loss: 0.4800 - val_AUC: 0.8880 - val_binary_accuracy: 0.8091\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4897 - AUC: 0.8787 - binary_accuracy: 0.7942 - val_loss: 0.4798 - val_AUC: 0.8884 - val_binary_accuracy: 0.8087\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4894 - AUC: 0.8794 - binary_accuracy: 0.7958 - val_loss: 0.4796 - val_AUC: 0.8889 - val_binary_accuracy: 0.8097\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4893 - AUC: 0.8796 - binary_accuracy: 0.7944 - val_loss: 0.4797 - val_AUC: 0.8889 - val_binary_accuracy: 0.8087\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4892 - AUC: 0.8796 - binary_accuracy: 0.7941 - val_loss: 0.4798 - val_AUC: 0.8898 - val_binary_accuracy: 0.8100\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4892 - AUC: 0.8802 - binary_accuracy: 0.7936 - val_loss: 0.4796 - val_AUC: 0.8898 - val_binary_accuracy: 0.8106\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4890 - AUC: 0.8802 - binary_accuracy: 0.7944 - val_loss: 0.4797 - val_AUC: 0.8903 - val_binary_accuracy: 0.8109\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4892 - AUC: 0.8805 - binary_accuracy: 0.7940 - val_loss: 0.4796 - val_AUC: 0.8893 - val_binary_accuracy: 0.8087\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 2s 4ms/step - loss: 0.6995 - AUC: 0.6937 - binary_accuracy: 0.6423 - val_loss: 0.6200 - val_AUC: 0.8117 - val_binary_accuracy: 0.7444\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5834 - AUC: 0.8426 - binary_accuracy: 0.7699 - val_loss: 0.5437 - val_AUC: 0.8702 - val_binary_accuracy: 0.7887\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5280 - AUC: 0.8759 - binary_accuracy: 0.8024 - val_loss: 0.5042 - val_AUC: 0.8892 - val_binary_accuracy: 0.8116\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5002 - AUC: 0.8888 - binary_accuracy: 0.8173 - val_loss: 0.4843 - val_AUC: 0.9002 - val_binary_accuracy: 0.8281\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4866 - AUC: 0.8961 - binary_accuracy: 0.8242 - val_loss: 0.4741 - val_AUC: 0.9053 - val_binary_accuracy: 0.8353\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4794 - AUC: 0.9007 - binary_accuracy: 0.8271 - val_loss: 0.4687 - val_AUC: 0.9088 - val_binary_accuracy: 0.8397\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4752 - AUC: 0.9043 - binary_accuracy: 0.8301 - val_loss: 0.4658 - val_AUC: 0.9114 - val_binary_accuracy: 0.8403\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4726 - AUC: 0.9060 - binary_accuracy: 0.8311 - val_loss: 0.4628 - val_AUC: 0.9143 - val_binary_accuracy: 0.8409\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4706 - AUC: 0.9081 - binary_accuracy: 0.8324 - val_loss: 0.4614 - val_AUC: 0.9170 - val_binary_accuracy: 0.8447\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4691 - AUC: 0.9106 - binary_accuracy: 0.8351 - val_loss: 0.4601 - val_AUC: 0.9179 - val_binary_accuracy: 0.8434\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4679 - AUC: 0.9117 - binary_accuracy: 0.8341 - val_loss: 0.4589 - val_AUC: 0.9194 - val_binary_accuracy: 0.8453\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4670 - AUC: 0.9131 - binary_accuracy: 0.8346 - val_loss: 0.4580 - val_AUC: 0.9215 - val_binary_accuracy: 0.8487\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4658 - AUC: 0.9149 - binary_accuracy: 0.8377 - val_loss: 0.4578 - val_AUC: 0.9215 - val_binary_accuracy: 0.8475\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4652 - AUC: 0.9159 - binary_accuracy: 0.8375 - val_loss: 0.4565 - val_AUC: 0.9233 - val_binary_accuracy: 0.8509\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4645 - AUC: 0.9171 - binary_accuracy: 0.8400 - val_loss: 0.4561 - val_AUC: 0.9238 - val_binary_accuracy: 0.8500\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4639 - AUC: 0.9179 - binary_accuracy: 0.8398 - val_loss: 0.4555 - val_AUC: 0.9256 - val_binary_accuracy: 0.8547\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4636 - AUC: 0.9189 - binary_accuracy: 0.8401 - val_loss: 0.4551 - val_AUC: 0.9260 - val_binary_accuracy: 0.8550\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4629 - AUC: 0.9204 - binary_accuracy: 0.8423 - val_loss: 0.4546 - val_AUC: 0.9261 - val_binary_accuracy: 0.8522\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4627 - AUC: 0.9198 - binary_accuracy: 0.8418 - val_loss: 0.4549 - val_AUC: 0.9280 - val_binary_accuracy: 0.8519\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4622 - AUC: 0.9218 - binary_accuracy: 0.8420 - val_loss: 0.4542 - val_AUC: 0.9275 - val_binary_accuracy: 0.8512\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4619 - AUC: 0.9217 - binary_accuracy: 0.8423 - val_loss: 0.4540 - val_AUC: 0.9290 - val_binary_accuracy: 0.8525\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4616 - AUC: 0.9232 - binary_accuracy: 0.8453 - val_loss: 0.4537 - val_AUC: 0.9291 - val_binary_accuracy: 0.8584\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4613 - AUC: 0.9228 - binary_accuracy: 0.8448 - val_loss: 0.4537 - val_AUC: 0.9306 - val_binary_accuracy: 0.8600\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4610 - AUC: 0.9243 - binary_accuracy: 0.8478 - val_loss: 0.4533 - val_AUC: 0.9305 - val_binary_accuracy: 0.8559\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4607 - AUC: 0.9246 - binary_accuracy: 0.8481 - val_loss: 0.4533 - val_AUC: 0.9314 - val_binary_accuracy: 0.8581\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4604 - AUC: 0.9255 - binary_accuracy: 0.8473 - val_loss: 0.4530 - val_AUC: 0.9311 - val_binary_accuracy: 0.8597\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4602 - AUC: 0.9258 - binary_accuracy: 0.8490 - val_loss: 0.4529 - val_AUC: 0.9326 - val_binary_accuracy: 0.8581\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4598 - AUC: 0.9266 - binary_accuracy: 0.8498 - val_loss: 0.4529 - val_AUC: 0.9324 - val_binary_accuracy: 0.8550\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4598 - AUC: 0.9271 - binary_accuracy: 0.8506 - val_loss: 0.4526 - val_AUC: 0.9330 - val_binary_accuracy: 0.8569\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4594 - AUC: 0.9275 - binary_accuracy: 0.8505 - val_loss: 0.4523 - val_AUC: 0.9336 - val_binary_accuracy: 0.8569\n"
     ]
    }
   ],
   "source": [
    "# hidden_sizes = [2, 5, 10, 20, 50]\n",
    "# l2_regs = [0.01, 0.001, 0.0001]\n",
    "# activations = ['relu', 'tanh']\n",
    "\n",
    "hidden_sizes = [2, 5]\n",
    "l2_regs = [0.01]\n",
    "activations = ['relu', 'tanh']\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "early_call = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', mode='max', patience=10, restore_best_weights=True)\n",
    "\n",
    "for activation in activations:\n",
    "    for l2_reg in l2_regs:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            if activation == 'relu':\n",
    "                activate = tf.keras.activations.relu\n",
    "            elif activation == 'tanh':\n",
    "                activate = tf.keras.activations.tanh\n",
    "\n",
    "            # Create Tensorboard Callback\n",
    "            param_string = 'act-{},l2-{},hs-{}'.format(activation, l2_reg, hidden_size)\n",
    "            log_dir = 'logs/binary_classification_test/' + param_string\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "            # Input layer\n",
    "            inputs = tf.keras.Input(shape=(X_train.shape[1]))\n",
    "\n",
    "            # Hidden layer with regularization and ReLU\n",
    "            hidden = tf.keras.layers.Dense(hidden_size, kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(inputs)\n",
    "            hidden = activate(hidden)\n",
    "\n",
    "            # Output layer with regularization and sigmoid\n",
    "            outputs = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(hidden)\n",
    "            outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=outputs, name='RegularizedModel')\n",
    "\n",
    "            model.compile(\n",
    "                    optimizer=tf.optimizers.Adam(),\n",
    "                    loss=tf.losses.BinaryCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.AUC(name='AUC'), tf.keras.metrics.BinaryAccuracy()],\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, y_dev),\n",
    "                      callbacks=[early_call, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 26528), started 0:03:36 ago. (Use '!kill 26528' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-72c7781098dc46fc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-72c7781098dc46fc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/binary_classification_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
